# Shared-Nothing 架构与 Fiber 并发模型

> Redis 的单线程模型简单但无法利用多核。常见的多线程方案（如加锁保护共享数据）又会引入锁竞争。NanoRedis 采用 Shared-Nothing 架构——每个线程拥有独立的数据分片，线程之间通过消息传递通信，既保留了"无锁"的优势，又实现了多核并行。

## 1. 从单线程到多线程的演进

### 1.1 Redis 的单线程模型

```
                ┌────────────────────────────┐
                │       Redis 进程           │
                │  ┌──────────────────────┐  │
                │  │   事件循环（单线程）    │  │
                │  │                      │  │
  Client A ────►│  │  accept / read /     │  │
  Client B ────►│  │  parse / execute /   │  │
  Client C ────►│  │  write              │  │
                │  │                      │  │
                │  │  ┌────────────────┐  │  │
                │  │  │   全部数据      │  │  │
                │  │  └────────────────┘  │  │
                │  └──────────────────────┘  │
                └────────────────────────────┘

优点: 无锁，实现简单，无竞态条件
缺点: 只能用 1 个 CPU 核心
```

### 1.2 加锁的多线程模型（不采用）

```
                ┌────────────────────────────┐
                │       多线程服务器          │
                │  ┌──────┐  ┌──────┐       │
                │  │线程 0 │  │线程 1 │ ...  │
                │  └───┬───┘  └───┬───┘      │
                │      │          │          │
                │      ▼          ▼          │
                │  ┌──────────────────────┐  │
                │  │  🔒 共享数据（加锁）   │  │
                │  └──────────────────────┘  │
                └────────────────────────────┘

优点: 利用多核
缺点: 锁竞争（高并发下性能急剧下降）
      死锁风险、难以调试
```

### 1.3 NanoRedis 的 Shared-Nothing 模型

```
                ┌─────────────────────────────────────────┐
                │           NanoRedis 进程                 │
                │                                         │
                │  ┌─────────┐  ┌─────────┐  ┌─────────┐ │
                │  │ vCPU 0  │  │ vCPU 1  │  │ vCPU 2  │ │
                │  │         │  │         │  │         │ │
                │  │ 连接    │  │ 连接    │  │ 连接    │ │
                │  │ ┌─────┐ │  │ ┌─────┐ │  │ ┌─────┐ │ │
                │  │ │分片 0│ │  │ │分片 1│ │  │ │分片 2│ │ │
                │  │ └─────┘ │  │ └─────┘ │  │ └─────┘ │ │
                │  └────┬────┘  └────┬────┘  └────┬────┘ │
                │       │            │            │      │
                │       └─── TaskQueue 消息传递 ───┘      │
                └─────────────────────────────────────────┘

优点: 无锁（数据不共享）+ 多核（每核一线程）
缺点: 跨分片请求需要消息传递（微秒级开销）
```

**核心思想**：与其保护共享数据，不如消除共享。每个线程拥有独立的数据分片，如果请求的键属于本线程的分片，直接本地执行；否则通过 TaskQueue 发送到目标线程。

## 2. 核心组件

### 2.1 组件总览

```
ShardedServer
  │
  ├── ProactorPool ─────────────── 线程管理
  │     │
  │     ├── vCPU Thread 0
  │     │     ├── Photon 事件循环
  │     │     ├── TCP Server (SO_REUSEPORT)
  │     │     ├── Connection Fiber × M
  │     │     └── Active Expire Fiber
  │     │
  │     ├── vCPU Thread 1 ... (同上)
  │     └── vCPU Thread N-1 ... (同上)
  │
  ├── EngineShardSet ───────────── 跨分片通信
  │     │
  │     ├── EngineShard 0
  │     │     ├── Database (16 个逻辑 DB)
  │     │     └── TaskQueue (无锁 MPMC)
  │     │
  │     ├── EngineShard 1 ... (同上)
  │     └── EngineShard N-1 ... (同上)
  │
  └── CommandRegistry ──────────── 命令元数据
```

### 2.2 ProactorPool：线程管理器

`ProactorPool` 是整个服务器的"引擎"，管理 N 个 vCPU 线程：

```cpp
class ProactorPool {
public:
    void Run(uint16_t port, uint32_t num_shards);
    void Stop();

    // 每个 vCPU 线程的主函数
    void VcpuMain(uint16_t port, uint32_t vcpu_index);

    // 处理单个客户端连接
    void HandleConnection(photon::net::ISocketStream* sock, uint32_t vcpu_index);
};
```

每个 vCPU 线程的启动流程：

```
VcpuMain(port, vcpu_index)
    │
    ├── photon::init()               // 初始化 Fiber 运行时
    │
    ├── EngineShard::InitCurrent()   // 创建本线程的数据分片
    │
    ├── new_tcp_socket_server()      // 创建 TCP 服务器
    │   └── bind + SO_REUSEPORT      // 内核级负载均衡
    │
    ├── start TaskQueue consumer      // 启动任务消费 Fiber
    │
    ├── start Active Expire Fiber    // 启动主动过期 Fiber
    │
    └── server->start_loop()         // 开始接受连接
        └── 每个新连接 → 创建 Connection Fiber
```

### 2.3 EngineShard：数据分片

`EngineShard` 是一个纯数据容器，使用线程本地存储（TLS）确保每个线程只能访问自己的分片：

```cpp
class EngineShard {
    Database db_;          // 16 个逻辑数据库
    TaskQueue task_queue_; // 接收跨分片请求的队列
    uint32_t shard_id_;    // 分片 ID

    static thread_local EngineShard* tlocal_shard_;  // 线程本地指针
};
```

**线程本地存储的好处**：

```cpp
// 访问当前线程的分片——零开销，无锁
EngineShard* shard = EngineShard::tlocal_shard_;
shard->db_.Get(key);  // 直接访问，无需锁
```

### 2.4 EngineShardSet：跨分片通信

`EngineShardSet` 提供两种跨分片通信方式：

```cpp
class EngineShardSet {
public:
    // 同步方式：发送任务到目标分片，挂起当前 Fiber 等待结果
    void Await(uint32_t shard_id, std::function<void()> func);

    // 异步方式：发送任务到目标分片，不等待结果
    void Add(uint32_t shard_id, std::function<void()> func);
};
```

**Await 的工作原理**：

```
vCPU X 的 Connection Fiber          vCPU Y 的 TaskQueue Consumer
            │                                    │
            │  1. 将任务打包                       │
            │     (func + semaphore)              │
            ├──────────────────────►              │
            │  2. Fiber 挂起                       │
            │     (线程 X 可以做其他事)             │
            │                           3. 取出任务 │
            │                           4. 执行 func│
            │                           5. 信号量 +1 │
            │                        ◄────────────│
            │  6. Fiber 被唤醒                      │
            │  7. 获取结果                          │
            ▼                                     ▼
```

**关键**：Await 挂起的是 Fiber，不是 OS 线程。线程在等待期间可以调度其他 Fiber（处理其他连接），不会浪费 CPU。

### 2.5 TaskQueue：无锁消息队列

TaskQueue 是一个无锁的 MPMC（多生产者多消费者）环形缓冲区：

```
         生产者（多个 vCPU 线程）
              │  │  │
              ▼  ▼  ▼
  ┌───┬───┬───┬───┬───┬───┬───┬───┐
  │   │   │ T │ T │ T │   │   │   │  环形缓冲区
  └───┴───┴───┴───┴───┴───┴───┴───┘
              ▲  ▲
              │  │
         消费者（目标 vCPU 的 Fiber）
```

**设计要点**：
- **无锁**：使用原子操作和序列号实现
- **Fiber 友好**：消费者空闲时通过 `photon::semaphore` 等待，不自旋
- **背压**：队列满时生产者 sleep 1ms 后重试
- **合并唤醒**：只在消费者空闲时发送唤醒信号

## 3. 请求路由

### 3.1 分片计算

每个键通过哈希函数映射到一个分片：

```cpp
uint32_t Shard(std::string_view key, uint32_t num_shards) {
    uint64_t hash = absl::Hash<std::string_view>{}(key);
    return hash % num_shards;
}
```

### 3.2 路由决策

当一个连接接收到命令时，路由逻辑如下：

```
收到命令: SET mykey myvalue
    │
    ▼
① CommandRegistry 查找命令元数据
   SET → { arity: -3, key_pos: 1, flags: write }
    │
    ▼
② 从参数中提取键
   args[key_pos] = args[1] = "mykey"
    │
    ▼
③ 计算目标分片
   target_shard = Hash("mykey") % num_shards
    │
    ▼
④ 路由决策
   ┌─────────────────────────────────┐
   │ target_shard == vcpu_index ?    │
   ├────────┬────────────────────────┤
   │   是   │         否             │
   │        │                        │
   │  本地   │  EngineShardSet::      │
   │  直接   │  Await(target_shard,   │
   │  执行   │        command_func)   │
   └────────┴────────────────────────┘
```

### 3.3 多键命令的处理

像 `MGET key1 key2 key3` 这样的多键命令，键可能分布在不同分片：

```
MGET key1 key2 key3

  key1 → shard 0
  key2 → shard 2
  key3 → shard 0

分组:
  shard 0: [key1, key3]  → 本地执行（如果当前在 shard 0）
  shard 2: [key2]        → Await 发送到 shard 2

结果聚合后返回客户端
```

## 4. Fiber 并发模型

### 4.1 为什么用 Fiber？

```
OS 线程模型（每连接一线程）:
  1 万连接 = 1 万线程
  每个线程 ~8MB 栈 = 80GB 内存 ← 不可行
  上下文切换开销巨大

Fiber 模型:
  1 万连接 = 1 万 Fiber
  每个 Fiber ~几 KB 栈 = 几十 MB 内存 ← 可行
  Fiber 切换在用户态，极快（~100ns）
```

### 4.2 每个 vCPU 上的 Fiber 调度

```
vCPU 线程 0（1 个 OS 线程）
┌──────────────────────────────────────────────┐
│  Photon 事件循环                              │
│                                              │
│  ┌─────────────────┐  当前运行的 Fiber         │
│  │ Connection      │  ◄── 处理客户端 A 的请求   │
│  │ Fiber A         │                          │
│  └────────┬────────┘                          │
│           │ 等待 I/O → 挂起                     │
│           ▼                                   │
│  ┌─────────────────┐                          │
│  │ Connection      │  ◄── 切换到客户端 B       │
│  │ Fiber B         │                          │
│  └────────┬────────┘                          │
│           │ 等待跨分片结果 → 挂起               │
│           ▼                                   │
│  ┌─────────────────┐                          │
│  │ TaskQueue       │  ◄── 处理其他分片发来的请求 │
│  │ Consumer Fiber  │                          │
│  └────────┬────────┘                          │
│           │ 队列为空 → 挂起                     │
│           ▼                                   │
│  ┌─────────────────┐                          │
│  │ Active Expire   │  ◄── 每 100ms 清理过期键  │
│  │ Fiber           │                          │
│  └─────────────────┘                          │
│                                              │
│  调度器会在 Fiber 挂起时自动切换到下一个就绪 Fiber │
└──────────────────────────────────────────────┘
```

**关键原则**：

1. **Fiber 只在显式等待时才会切换**（I/O、semaphore、sleep），不会被抢占
2. **同一 OS 线程上的所有 Fiber 共享地址空间**，无需同步
3. **单个 OS 线程可以运行数千个 Fiber**，每个 Fiber 代表一个连接或任务

### 4.3 PhotonLibOS 集成

NanoRedis 使用 PhotonLibOS 作为 Fiber 运行时：

```
PhotonLibOS 提供:

photon::init()                    初始化当前线程的事件循环
photon::thread_create11()         创建 Fiber
photon::thread_usleep()           Fiber 级别的 sleep
photon::semaphore                 Fiber 友好的信号量
photon::net::new_tcp_socket_server()  创建 TCP 服务器
photon::net::ISocketStream        Socket 抽象
```

## 5. SO_REUSEPORT 负载均衡

### 5.1 传统方案的问题

```
传统方案：单一 accept 线程

       Client A ─┐
       Client B ─┤    ┌──────────────┐     ┌──────────┐
       Client C ─┼───►│ Accept 线程   │────►│ Worker 0 │
       Client D ─┤    │ （瓶颈！）    │     │ Worker 1 │
       Client E ─┘    └──────────────┘     │ Worker 2 │
                                           └──────────┘
       accept 线程成为瓶颈，且需要额外的连接分发逻辑
```

### 5.2 SO_REUSEPORT 方案

```
SO_REUSEPORT 方案：每个 vCPU 独立 accept

       Client A ──►┌──────────────┐
       Client B ──►│ vCPU 0       │  独立 TCP Server
                   │ (bind:9527)  │  独立 accept
                   └──────────────┘

       Client C ──►┌──────────────┐
       Client D ──►│ vCPU 1       │  独立 TCP Server
                   │ (bind:9527)  │  独立 accept
                   └──────────────┘

       Client E ──►┌──────────────┐
                   │ vCPU 2       │  独立 TCP Server
                   │ (bind:9527)  │  独立 accept
                   └──────────────┘

       内核负责将新连接均匀分配到各个 vCPU
       无单点瓶颈，无需用户态负载均衡
```

多个 socket 绑定同一端口，内核自动将新连接分配到不同 socket。每个 vCPU 独立 accept，消除了 accept 线程的瓶颈。

## 6. Connection Fiber 的生命周期

每个客户端连接由一个 Connection Fiber 管理，它的完整生命周期如下：

```
TCP Accept（新连接到达）
    │
    ▼
创建 Connection 对象
    │  - socket
    │  - RESPParser
    │  - write_buffer
    │  - db_index = 0
    │
    ▼
创建 Connection Fiber ──────────────────── 进入事件循环
    │
    ▼
┌──────────────────────────────────┐
│          主循环                   │
│                                  │
│  ① ParseCommand()               │
│     从 socket 读取并解析 RESP     │
│     │                            │
│  ② 查找命令元数据                 │
│     CommandRegistry::FindMeta()  │
│     │                            │
│  ③ 路由到目标分片                 │
│     本地执行 或 Await()           │
│     │                            │
│  ④ 将响应追加到 write_buffer     │
│     │                            │
│  ⑤ 尝试 Pipeline               │
│     TryParseCommandNoRead()     │
│     ├── 有更多命令 → 回到 ②      │
│     └── 无更多命令 → flush 并     │
│         回到 ① 等待新数据         │
│                                  │
└──────────────────────────────────┘
    │
    ▼ （socket 关闭 / 错误）
清理 Connection 对象
Fiber 退出
```

## 7. 性能特性

### 7.1 本地请求的性能优势

当键恰好在当前 vCPU 的分片上时（本地请求），整个流程没有任何跨线程通信：

```
本地请求延迟:
  网络读取: ~1μs (io_uring)
  RESP 解析: ~100ns
  DashTable 查找: ~50ns
  网络写入: ~1μs (缓冲)
  ──────────────────
  总计: ~2-3μs
```

### 7.2 跨分片请求的额外开销

```
跨分片请求额外开销:
  TaskQueue 入队: ~100ns (原子操作)
  Fiber 切换: ~100ns
  信号量唤醒: ~200ns
  ──────────────────
  额外开销: ~1-2μs
```

### 7.3 线性可扩展性

在没有热点键的理想情况下：

```
1 个分片:  ~10 万 ops/s
2 个分片:  ~20 万 ops/s
4 个分片:  ~40 万 ops/s
N 个分片:  ~N × 10 万 ops/s
```

## 8. 设计权衡

### 8.1 优势

| 优势 | 说明 |
|------|------|
| 无锁 | 数据路径上没有任何锁 |
| 缓存友好 | 数据保持在同一 CPU 的缓存中 |
| 线性扩展 | 增加 CPU 核心 = 增加吞吐量 |
| 可预测延迟 | 没有全局锁争用导致的延迟抖动 |

### 8.2 代价

| 代价 | 说明 | 缓解措施 |
|------|------|---------|
| 跨分片延迟 | 请求的键不在本地分片时有额外开销 | 统计上约 (N-1)/N 的请求需要跨分片，但开销仅 ~1-2μs |
| 热点问题 | 热点键集中在一个分片 | 可通过键的哈希分布来缓解 |
| 多键原子性 | 跨分片的多键命令无法原子执行 | MGET/MSET 通过分组 + 聚合来处理 |
| 内存碎片 | 每个分片独立分配内存 | 使用 mimalloc 减少碎片 |

## 9. 配置

```bash
# 启动 4 分片服务器
./build/nano_redis_server --num_shards=4 --port=9527

# 单分片（退化为类 Redis 单线程模式）
./build/nano_redis_server --num_shards=1 --port=9527
```

- `--num_shards`：分片数量，建议等于 CPU 核心数（默认 8）
- `--port`：监听端口（默认 9527）

---

上一篇：[DashTable：增量扩容的高性能哈希表](2.dashtable.md)
下一篇：[RESP 协议解析与 Pipeline 批处理](4.resp_and_pipeline.md)
