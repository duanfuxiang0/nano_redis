# NanoRedis 持久化

---

## 1. NanoRedis 持久化架构 

NanoRedis 的持久化系统由四层组成：

```
┌────────────────────────────────────────────────────────────┐
│              命令入口层 (ServerFamily)                       │
│  SAVE / BGSAVE  →  DoSave()  →  SaveStagesController      │
├────────────────────────────────────────────────────────────┤
│              协调层 (SaveStagesController)                  │
│  管理整个快照生命周期：Init → Start → Wait → Finalize       │
│  支持两种模式：单文件 RDB / 多文件 DFS                       │
├────────────────────────────────────────────────────────────┤
│              分片快照层 (SliceSnapshot)                      │
│  每个 shard 线程内独立运行，遍历 DashTable 序列化数据        │
│  通过 version 机制保证 point-in-time 一致性                  │
├────────────────────────────────────────────────────────────┤
│              序列化层 (RdbSerializer)                       │
│  将 K/V 编码为 RDB 二进制格式，支持压缩                      │
└────────────────────────────────────────────────────────────┘
```

涉及的核心源文件：

| 文件 | 职责 |
|------|------|
| `server_family.cc` | `SAVE`/`BGSAVE` 命令入口 |
| `detail/save_stages_controller.h/cc` | 快照全局协调器 |
| `snapshot.h/cc` | 每个 shard 的快照逻辑（`SliceSnapshot`） |
| `rdb_save.h/cc` | RDB 格式序列化（`RdbSaver` + `RdbSerializer`） |
| `rdb_load.h/cc` | RDB 格式反序列化（`RdbLoader`） |
| `detail/snapshot_storage.h/cc` | 存储后端抽象（本地文件 / S3 / GCS） |

---

## 2. 核心机制深度分析

### 2.1 Forkless 快照——为什么不用 fork

Redis 使用 `fork()` + copy-on-write 实现 BGSAVE。这有几个问题：

1. **内存翻倍风险**：写密集场景下 CoW 可能导致几乎所有页面被复制
2. **fork 延迟**：大内存进程 fork 本身就很慢（需要复制页表）
3. **与 shared-nothing 不兼容**：多线程进程 fork 后子进程只有一个线程，无法正确遍历所有 shard

Dragonfly 的做法是 **forkless snapshotting**——利用 DashTable 的 bucket version 机制，在不停服的情况下实现一致性快照。

### 2.2 Point-in-Time 一致性（Version 机制）

这是 Dragonfly 快照最精妙的设计。核心思想：

```
快照开始时，捕获一个全局"切割点"（epoch）
每个 bucket 有 version 字段
version < epoch 的数据 → 还没被序列化 → 需要保存
version >= epoch 的数据 → 已经被序列化或者是快照后新写入 → 跳过
```

**IterateBucketsFb（遍历 Fiber）**——在 shard 线程中启动一个 Fiber，逐 bucket 遍历：

```cpp
for (entry : table) {
    if (entry.version <= cut.epoch) {
        entry.version = cut.epoch + 1;  // 标记已序列化
        SendToSerializationSink(entry);
    }
}
```

**OnDbChange（写入钩子）**——对每次写操作拦截，如果要修改的 bucket 还没被扫到，先序列化旧值：

```cpp
if (entry.version <= cut.version) {
    SendToSerializationSink(entry);  // 先保存旧值
}
entry = new_entry;
entry.version = shard.epoch++;  // 新 version > cut，不会被重复序列化
```

这保证了即使快照过程中有并发写入，快照内容也是"快照开始那一刻"的精确状态。

### 2.3 数据流：从 shard 到磁盘

Dragonfly 支持两种落盘模式，数据流有所不同：

**模式一：单文件 RDB（Redis 兼容格式）**

```
vCPU 0 ──┐
vCPU 1 ──┤   SliceSnapshot     RdbSerializer    StringFile
vCPU 2 ──┤── 各 shard 独立  →  序列化为 RDB  →  写入字符串  ──→ Channel
vCPU 3 ──┘   遍历 DashTable    二进制格式       (内存缓冲)        │
                                                                  │ MPMC
                                                                  ▼
                                                          SaveBody Fiber
                                                                  │
                                                           AlignedBuffer
                                                           (对齐 4KB)
                                                                  │
                                                                  ▼
                                                            磁盘文件 .rdb
```

关键设计：
- 多个 shard 并行序列化，通过一个 **MPMC Channel** 汇聚到单个消费者 Fiber
- 每次推送的数据以 **bucket 为粒度**，保证数据的自包含性
- 使用 **AlignedBuffer** 对齐到 4KB 边界，支持 Direct I/O

**模式二：多文件 DFS（Dragonfly 原生格式）**

```
vCPU 0 → SliceSnapshot → RdbSerializer → shard-0000.dfs
vCPU 1 → SliceSnapshot → RdbSerializer → shard-0001.dfs
vCPU 2 → SliceSnapshot → RdbSerializer → shard-0002.dfs
vCPU 3 → SliceSnapshot → RdbSerializer → shard-0003.dfs
                                         + summary.dfs (元数据)
```

关键设计：
- 每个 shard 直接写自己的文件，**无需汇聚 Channel**，吞吐更高
- summary 文件存全局元数据（Lua 脚本、索引等）
- 加载时可以并行加载各 shard 文件

### 2.4 SaveStagesController 协调流程

```
ServerFamily::DoSave()
    │
    ▼
SaveStagesController::Init()
    ├── BuildFullPath() ── 构建文件路径
    ├── 创建 RdbSnapshot 数组：
    │   RDB 模式: 1 个 snapshot
    │   DFS 模式: N+1 个 snapshot (N shards + 1 summary)
    │
    ▼
SaveStagesController::Start()
    ├── [RDB] SaveRdb()
    │       ├── snapshot->Start(SaveMode::RDB, ...)  ── 打开文件、写 header
    │       └── trans->ScheduleSingleHop(cb)          ── 在每个 shard 启动 SliceSnapshot
    │
    ├── [DFS] SaveDfs()
    │       ├── SaveDfsSingle(nullptr, ...)           ── summary 文件
    │       └── trans->ScheduleSingleHop(cb)          ── 每个 shard 各自启动
    │
    ▼
SaveStagesController::WaitAllSnapshots()
    ├── [DFS] RunBlockingInParallel(WaitSnapshotInShard)  ── 各 shard 等待完成
    └── [RDB] SaveBody(0) ── 消费 Channel 写入文件
    │
    ▼
SaveStagesController::Finalize()
    ├── RunStage(CloseCb) ── 关闭文件、收集统计
    └── FinalizeFileMovement() ── .tmp → 正式文件（原子 rename）
```

### 2.5 RDB 二进制格式（简化版）

```
┌─────────────────────────────────────────┐
│ Magic: "REDIS0011" (9 bytes)            │  ← RDB 版本号
├─────────────────────────────────────────┤
│ AUX fields (redis-ver, ctime, etc.)     │  ← 元数据
├─────────────────────────────────────────┤
│ SELECTDB opcode + db_number             │  ← 选择数据库
├─────────────────────────────────────────┤
│ RESIZE_DB opcode + db_size + expire_size│  ← 预分配提示
├─────────────────────────────────────────┤
│ Entry: [EXPIRE opcode + timestamp]      │  ← 可选的过期时间
│        + type + key + value             │  ← 实际数据
│ Entry: ...                              │
│ ...                                     │
├─────────────────────────────────────────┤
│ EOF opcode                              │
│ 8-byte checksum (CRC64)                 │
└─────────────────────────────────────────┘
```

RdbSerializer 中每个 entry 的序列化：

```cpp
// rdb_save.cc: RdbSerializer::SaveEntry()
io::Result<uint8_t> SaveEntry(key, value, expire_ms, mc_flags, dbid) {
    SelectDb(dbid);                       // 切换 DB（仅在 dbid 变化时写入）
    if (expire_ms > 0)
        WriteOpcode(RDB_OPCODE_EXPIRETIME_MS) + SaveLen(expire_ms);
    uint8_t rdb_type = RdbObjectType(pv);
    WriteOpcode(rdb_type);                // 写类型标记
    SaveString(key);                      // 写 key
    SaveValue(value);                     // 写 value（按类型分派）
}
```

---

## 3. NanoRedis 适配方案

### 3.1 当前状态分析

NanoRedis 已经具备了实现持久化的关键基础设施：

| 已有能力 | 对应组件 | 持久化中的作用 |
|---------|---------|--------------|
| Shared-Nothing 分片 | `EngineShard` / `EngineShardSet` | 每个 shard 可独立做快照 |
| DashTable 遍历 | `DashTable::ForEach()` | 遍历所有 K/V 进行序列化 |
| 跨 shard 通信 | `TaskQueue::Await()` | 协调各 shard 的快照启停 |
| Fiber 并发 | PhotonLibOS | 快照可在独立 Fiber 中运行 |
| NanoObj 值对象 | 支持 String/Int/Hash/Set/List/Zset | 需要逐类型序列化 |
| 过期表 | `DashTable<NanoObj, int64_t>` | 需要一起序列化 |

**缺失的部分**：序列化器、文件 I/O、协调器、加载器。

### 3.2 推荐方案：分层实现

建议分三个阶段递进实现：

#### Phase 1：最小可用——同步 SAVE（阻塞式快照）

目标：实现 `SAVE` 命令，阻塞所有写入，逐 shard 序列化到单个文件。

```
SAVE 命令
    │
    ▼
阻塞所有 shard 的写入（暂停 TaskQueue 消费）
    │
    ▼
在协调线程上依次遍历每个 shard：
    shard_set->Await(shard_id, [&] {
        shard->GetDB().ForEachTable([&](db_index, key, value) {
            serializer.SaveEntry(key, value, expire, db_index);
        });
    });
    │
    ▼
写入文件，恢复服务
```

需要新增的组件：

```
include/core/rdb_serializer.h     ← RDB 二进制编码
src/core/rdb_serializer.cc

include/core/rdb_loader.h         ← RDB 二进制解码
src/core/rdb_loader.cc

include/server/save_controller.h   ← 快照协调
src/server/save_controller.cc
```

**RdbSerializer 核心接口（简化版）**：

```cpp
class RdbSerializer {
public:
    explicit RdbSerializer(io::Sink* sink);

    std::error_code SaveHeader();
    std::error_code SaveSelectDb(uint32_t dbid);
    std::error_code SaveEntry(const NanoObj& key, const NanoObj& value,
                              int64_t expire_ms, uint32_t dbid);
    std::error_code SaveFooter();

private:
    std::error_code SaveString(std::string_view val);
    std::error_code SaveLen(uint64_t len);
    std::error_code SaveObject(const NanoObj& obj);  // 按 type 分派
    std::error_code SaveHashObject(const NanoObj& obj);
    std::error_code SaveSetObject(const NanoObj& obj);
    std::error_code SaveListObject(const NanoObj& obj);
    std::error_code SaveZsetObject(const NanoObj& obj);

    std::error_code WriteOpcode(uint8_t opcode);
    std::error_code WriteRaw(const uint8_t* buf, size_t len);

    io::Sink* sink_;
    uint32_t last_dbid_ = UINT32_MAX;
    uint64_t checksum_ = 0;
};
```

**Database 需要新增遍历接口**：

```cpp
// database.h 新增
template <typename Func>
void ForEachInDB(size_t db_index, Func&& func) const {
    if (!tables[db_index]) return;
    tables[db_index]->ForEach([&](const NanoObj& key, const NanoObj& value) {
        int64_t expire_ms = 0;
        if (auto* exp = expire_tables[db_index].get()) {
            if (auto* p = exp->Find(key)) {
                expire_ms = *p;
            }
        }
        func(key, value, expire_ms);
    });
}
```

#### Phase 2：异步快照——BGSAVE（非阻塞 Fiber 快照）

目标：在后台 Fiber 中做快照，不阻塞正常请求。

这是 Dragonfly 架构的精华所在。核心是给 DashTable 的 Segment 加上 **version** 字段：

```cpp
// dashtable.h 改造
struct Segment {
    ankerl::unordered_dense::map<K, V> table;
    uint8_t local_depth;
    uint32_t segment_id;
    uint64_t version = 0;  // ← 新增：用于快照一致性
};
```

然后实现 `SliceSnapshot`：

```
┌──────────────────────────────────────────────────────┐
│                  SliceSnapshot (per shard)            │
│                                                      │
│  Start():                                            │
│    snapshot_version = current_epoch++                 │
│    注册 OnDbChange 回调                                │
│    启动 IterateBucketsFb fiber                         │
│                                                      │
│  IterateBucketsFb():                                 │
│    for each segment in DashTable:                    │
│      if segment.version < snapshot_version:          │
│        serialize all entries in segment              │
│        segment.version = snapshot_version            │
│      yield to other fibers periodically              │
│                                                      │
│  OnDbChange(key):  // 写操作钩子                      │
│    找到 key 所在 segment                              │
│    if segment.version < snapshot_version:            │
│      先序列化整个 segment（保存旧值）                    │
│      segment.version = snapshot_version              │
│    然后正常执行写操作                                   │
└──────────────────────────────────────────────────────┘
```

**为什么以 Segment 为粒度而不是单个 key**：
- DashTable 的 `ForEach` 以 Segment 为单位遍历
- Segment 级 version 避免了逐 key 追踪的开销
- 保证了 Segment 内数据的原子性

#### Phase 3：多文件 DFS 模式 + 启动加载

每个 shard 直接写各自的文件，最大化并行度：

```
nano_redis_data/
├── dump-0000.nrdb       ← shard 0
├── dump-0001.nrdb       ← shard 1
├── dump-0002.nrdb       ← shard 2
├── dump-0003.nrdb       ← shard 3
└── dump-summary.nrdb    ← 元数据（shard 数量、时间戳等）
```

启动时的加载流程：

```
server 启动
    │
    ▼
检测是否存在 dump 文件
    │
    ▼
读取 summary 文件，获取 shard 数量
    │
    ▼
各 shard 并行加载各自的文件：
    shard_set->Await(shard_id, [&] {
        RdbLoader loader(file_path);
        loader.Load([&](dbid, key, value, expire) {
            shard->GetDB().Set(key, value);
            if (expire > 0) shard->GetDB().Expire(key, expire);
        });
    });
```

### 3.3 实现优先级建议

```
                    复杂度    价值     建议
Phase 1 (SAVE)      ★★☆     ★★★    先做——最小可用，验证序列化正确性
Phase 2 (BGSAVE)    ★★★★   ★★★★  核心——生产可用的非阻塞快照
Phase 3 (DFS+Load)  ★★★     ★★★    增强——提高大数据量的落盘/加载速度
```

### 3.4 Phase 1 需要新增/修改的文件清单

```
新增文件:
├── include/core/rdb_serializer.h      # RDB 编码器
├── src/core/rdb_serializer.cc
├── include/core/rdb_loader.h          # RDB 解码器
├── src/core/rdb_loader.cc
├── include/core/rdb_defs.h            # RDB 常量定义 (opcodes, types)

修改文件:
├── include/core/database.h            # 新增 ForEachInDB 遍历接口
├── include/command/server_family.h    # 新增 Save/BgSave 命令声明
├── src/command/server_family.cc       # 实现 Save/BgSave + 启动加载
├── src/server/sharded_server.cc       # 启动时检测并加载 dump 文件
├── CMakeLists.txt                     # 添加新文件
```

---

## 4. 关键设计决策

### 4.1 是否兼容 Redis RDB 格式？

| 选项 | 优点 | 缺点 |
|------|------|------|
| **兼容 Redis RDB** | 可用 redis-cli 加载；生态兼容 | 格式复杂，需处理所有 Redis 类型编码 |
| **自定义简化格式** | 实现简单；可针对 NanoObj 优化 | 不兼容 Redis 生态 |

**建议**：Phase 1 采用自定义简化格式（`.nrdb`），快速验证。格式设计参考 RDB 但大幅简化——只需支持 NanoObj 的 5 种类型。后续如有需要，再加 RDB 兼容层。

### 4.2 自定义格式设计草案

```
文件结构:
┌──────────────────────────────────┐
│ Magic: "NRDB0001" (8 bytes)     │
├──────────────────────────────────┤
│ Metadata:                        │
│   shard_id (u32)                 │
│   num_shards (u32)               │
│   timestamp (u64)                │
│   num_dbs (u16)                  │
├──────────────────────────────────┤
│ Per-DB block:                    │
│   DB_SELECT opcode + db_index    │
│   DB_SIZE + key_count            │
│   Entry* (重复)                  │
├──────────────────────────────────┤
│ EOF opcode                       │
│ CRC32 checksum (4 bytes)         │
└──────────────────────────────────┘

Entry 编码:
┌──────────────────────────────────┐
│ [EXPIRE_MS opcode + i64]  可选   │  ← 有过期时间才写
├──────────────────────────────────┤
│ type_byte:                       │
│   0x00 = STRING                  │
│   0x01 = INT                     │
│   0x02 = HASH                   │
│   0x03 = SET                    │
│   0x04 = LIST                   │
│   0x05 = ZSET                   │
├──────────────────────────────────┤
│ key: len(varint) + bytes         │
├──────────────────────────────────┤
│ value: (按 type 分派)            │
│   STRING: len(varint) + bytes    │
│   INT: i64 (8 bytes fixed)       │
│   HASH: count(varint) +          │
│         (field_len + field +     │
│          value_len + value)*     │
│   SET: count(varint) +           │
│        (member_len + member)*    │
│   LIST: count(varint) +          │
│         (elem_len + elem)*       │
│   ZSET: count(varint) +          │
│         (member_len + member +   │
│          score as f64)*          │
└──────────────────────────────────┘
```

### 4.3 文件写入安全性

参照 Dragonfly 的做法：
1. 先写入 `.tmp` 临时文件
2. 完成后 `fsync()` 刷盘
3. 原子 `rename()` 为正式文件名
4. 失败时删除 `.tmp`，不影响已有的 dump 文件

---

## 5. 总结

Dragonfly 的持久化设计核心亮点：

1. **Forkless**：不用 fork，通过 version 机制实现快照一致性
2. **Per-shard 并行**：每个 shard 独立序列化，天然匹配 shared-nothing 架构
3. **Bucket 粒度**：序列化以 bucket/segment 为单位，保证原子性
4. **写入钩子**：快照期间的并发写通过 OnDbChange 回调保序
5. **Channel 汇聚**：多 shard 数据通过 MPMC Channel 汇到单文件（RDB 模式）

NanoRedis 的 shared-nothing 架构与 Dragonfly 高度相似，可以直接复用这套设计思路。建议从 Phase 1（阻塞式 SAVE）开始，逐步演进到 Phase 2（非阻塞 BGSAVE）。
