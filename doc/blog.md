# 用 C++ 写一个内存数据库：NanoRedis 开发手记

作为一个对数据库底层感兴趣的开发者，我一直想真正理解 Redis 这类内存数据库是如何工作的。看源码当然是一种方式，但 Redis 经过十几年的迭代，代码量庞大，很多设计决策的来龙去脉已经不那么清晰。于是我决定自己动手写一个——NanoRedis。这篇文章记录了我在这个过程中的思考和实践。


## 从数据结构开始

写任何程序，第一步都是数据结构的设计。对于内存数据库来说，核心数据结构毫无疑问是 HashTable——它让我们能用 O(1) 的时间把一个 key 映射到内存中的某个位置。但 HashTable 的实现有两个经典挑战：**如何解决哈希冲突**，以及**如何处理动态扩容**。

Redis 采用的是非常标准的方案：用拉链法解决冲突，用双字典切换的方式做渐进式 rehash。这是经过多年优化的成熟实现，性能相当不错。但既然是学习项目，我想探索一些不同的做法。

拉链法的问题在于，当负载因子升高时，每个桶的链表变长，遍历链表会带来额外的 cache miss。现代 CPU 对连续内存的访问有很强的优化，但链表节点散落在堆内存的各处，对 cache 非常不友好。双字典切换的 rehash 方案虽然把扩容的开销分摊到了多次操作中，但在切换期间需要维护两张表，内存压力翻倍，而且每次访问都要查两张表。

我想要的效果是这样的：首先，数据结构应该是"整齐"的——尽量使用连续的数组而不是指针和链表，让 CPU 的预取机制能够发挥作用。其次，扩容不应该是全局事件，而应该只影响局部的一小部分数据。

最终我选择了 `ankerl::unordered_dense` 作为底层的哈希表实现。这是一个类似 Google Abseil `flat_hash_map` 的开放寻址哈希表，所有数据都存储在连续的数组中，没有链表。但这里有个关键：我只用它来解决哈希冲突，不用它的自动扩容功能。每个 `unordered_dense` 实例在创建时就固定大小，不再改变。

那扩容怎么办？这就是 DashTable 的设计思路了——我借鉴了 DragonflyDB 的多 Segment 设计。整体思路是把数据分成多个 Segment，每个 Segment 是一个固定大小的 `unordered_dense` 实例。上层用一个 Directory 来索引这些 Segment。当某个 Segment 快满了，就把它分裂成两个，只需要移动这个 Segment 内的数据，其他 Segment 完全不受影响。

具体实现用到了 Global Depth 和 Local Depth 两个概念。Global Depth 决定 Directory 的大小（2^global_depth 个槽位），Local Depth 表示某个 Segment 被分裂过多少次。查找时，取 key 的哈希值的高位作为 Directory 索引，定位到对应的 Segment，然后在 Segment 内部用 `unordered_dense` 完成实际的查找。扩容时，只分裂那一个 Segment，复杂度从 O(n) 降到 O(k)，其中 k 是单个 Segment 的大小。


## 让每个字节都物尽其用

数据结构确定之后，下一个问题是：如何表示存储在 HashTable 中的对象？

Redis 用 `robj` 结构来表示对象，包含类型、编码、引用计数、指向实际数据的指针等字段。这个设计很灵活，但对于高密度的内存存储来说，每个对象的开销有些浪费。如果你有一百万个小字符串，每个字符串本身只有几个字节，但每个 `robj` 就要占用 16-24 字节，加上指针指向的 SDS 结构体开销，实际内存使用量会远超数据本身。

我设计了 NanoObj，目标是把一个通用的值对象压缩到固定的 16 字节。这个大小是刻意选择的——16 字节正好是很多 CPU 架构上一个 cache line 的 1/4 到 1/8，DashTable 中相邻的几个 NanoObj 可以完整地放进同一个 cache line。

NanoObj 的布局是这样的：1 字节的 tag（类型标记），1 字节的 flag（保留），剩下 14 字节是一个 union。对于短字符串（14 字节以内），直接内联存储在这 14 字节里，tag 的值就是字符串长度。对于整数，直接存储 int64_t 的值。对于超过 14 字节的字符串，使用 SmallString 结构：8 字节指针指向堆内存，2 字节长度，4 字节前缀。这个 4 字节前缀是个小优化——在哈希表中比较两个字符串时，先比较前缀，如果前缀不同就可以快速返回不相等，避免访问堆内存。

对于 key，还有一个额外的优化：如果 key 是纯数字字符串（比如用户 ID "12345"），就自动转换成整数存储。整数比较比字符串比较快得多，而且不需要堆分配。这个优化在实际场景中收益很大，因为很多业务的 key 就是各种 ID。

复合类型（Hash、Set、List、ZSet）用 RobjWrapper 表示，14 字节里放一个 8 字节指针指向实际的数据结构，4 字节的 size，加上 type 和 encoding 标记。这些复合类型的实际数据结构可以按需选择——Hash 用 DashTable，List 用 deque，等等。


## 驾驭多核

数据结构和对象表示解决之后，就到了最有挑战性的部分：并发。

Redis 最初的设计是单线程的。一个主线程同时负责网络 IO、命令解析、数据操作，通过 epoll 事件循环实现多用户并发。单线程的优势是简单——不需要任何锁，没有竞态条件。近年来 Redis 开始引入 IO 多线程，把网络读写分到多个线程，但内存数据操作仍然是单线程的。这意味着即使你的服务器有 64 个核心，Redis 实际只用到其中一个来处理命令。

我想探索的是如何真正利用多核。最直接的想法是加锁——用一把大锁保护整个数据结构，每次操作前加锁，操作后解锁。但这基本上等于把多核退化成单核，锁本身的开销还不小。细粒度锁（比如每个 key 一把锁）可以提高并发度，但实现复杂，而且分片之间如果有依赖（比如 MGET 这种多 key 命令），容易死锁。

我最终选择了 Shared-Nothing 架构，这也是 DragonflyDB 的核心设计。思路是这样的：把数据分成 N 个 Shard（通常等于 CPU 核心数），每个 Shard 由一个专门的线程负责。Shard 之间不共享任何可变状态。既然不共享，自然就不需要锁。

但这带来了一个新问题：网络连接是随机到达的，一个客户端的请求可能访问任意一个 Shard 的数据。如果请求到达的线程不是目标 Shard 的归属线程，怎么办？

解决方案是**任务队列**。每个 Shard 有一个 MPSC（多生产者单消费者）队列。当一个线程收到请求，解析出 key，计算出目标 Shard 后，如果目标不是自己，就把任务打包投递到目标 Shard 的队列里，然后挂起当前协程。目标线程从队列中取出任务执行，完成后通知发起方。注意这里是协程挂起而不是线程阻塞——底层我用了 Photon 这个协程库，它封装了 epoll 和 io_uring，支持轻量级协程。一个线程可以同时运行成千上万个协程，协程切换的开销远小于线程切换。所以即使某个请求需要等待另一个 Shard，当前线程也不会闲着，可以继续处理其他请求。

每个线程还各自监听同一个端口，用 `SO_REUSEPORT` 选项让内核来做负载均衡。这样就避免了传统的单一 accept 线程成为瓶颈。

这套设计的效果是：对于单 key 操作（绝大多数请求），直接在本地 Shard 处理，零锁零队列；对于跨 Shard 操作，有微秒级的队列开销，但线程不阻塞。随着核心数增加，吞吐量接近线性增长。


## 写在最后

做这个项目，最有价值的不是最终的代码，而是过程中对各种设计决策的理解。为什么 Redis 用拉链法？因为实现简单，而且在 Redis 诞生的年代，CPU 和内存的差距没有现在这么大。为什么 DragonflyDB 要搞 Shared-Nothing？因为锁的开销在多核时代变得不可接受。这些"为什么"在看源码时很难体会，只有自己踩过坑才能真正理解。

NanoRedis 的代码开源在 GitHub，希望对同样对内存数据库感兴趣的人有所帮助。如果你有任何问题或建议，欢迎交流。
